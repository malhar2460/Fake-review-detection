{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries & dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\MALHAR\n",
      "[nltk_data]     PRAJAPATI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\MALHAR\n",
      "[nltk_data]     PRAJAPATI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\MALHAR\n",
      "[nltk_data]     PRAJAPATI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler, LabelEncoder\n",
    "from langdetect import detect\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "import inflect\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import contractions\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Languages used in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages found  ['en' 'de' 'et' 'es' 'da' 'fr' 'vi' 'so' 'ca' 'af' 'pt' 'nl' 'no' 'sw'\n",
      " 'tl' 'unknown']\n",
      "Total instances where it was not english 64\n"
     ]
    }
   ],
   "source": [
    "def func(text):\n",
    "    try:\n",
    "        if isinstance(text, str) and text.strip():\n",
    "            return detect(text)\n",
    "        else:\n",
    "            return \"unknown\"\n",
    "    except:\n",
    "        return \"unknown\"  \n",
    "\n",
    "temp = df['text_'].apply(func)\n",
    "\n",
    "print(\"Languages found \",temp.unique())\n",
    "print(\"Total instances where it was not english\",temp[temp != 'en'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The above code return different output everytime but still since there is more than one language it is enough for us do translation regardless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To translate non-English reviews into English using the `deep_translator` library while ensuring progress is saved to avoid reprocessing previously translated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "output_file = \"../Data/Pre-processed/translated_output.csv\" \n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    df_translated = pd.read_csv(output_file)\n",
    "    translated_indices = set(df_translated.index)  \n",
    "else:\n",
    "    df_translated = pd.DataFrame()\n",
    "    translated_indices = set()\n",
    "\n",
    "def deep_translate(text):\n",
    "    try:\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return \"Invalid or Empty Text\"\n",
    "        return GoogleTranslator(source='auto', target='en').translate(text)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Deep Translator Error: {e}\")\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if i in translated_indices:\n",
    "        continue  \n",
    "    df.loc[i, 'deep_translated_text'] = deep_translate(df.loc[i, 'text_'])\n",
    "    df.iloc[[i]].to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)\n",
    "    print(f\"Processed row {i+1}/{len(df)}: {df.loc[i, 'deep_translated_text']}\")  \n",
    "\n",
    "print(\"Translation completed or interrupted, progress saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages found  ['en' 'de' 'af' 'et' 'da' 'so' 'fr' 'ca' 'nl' 'vi' 'no' 'sw' 'tl'\n",
      " 'unknown']\n",
      "Total instances where it was not english 36\n"
     ]
    }
   ],
   "source": [
    "trans_df = pd.read_csv(\"../Data/Pre-processed/translated_output.csv\")\n",
    "def func(text):\n",
    "    try:\n",
    "        if isinstance(text, str) and text.strip():\n",
    "            return detect(text)\n",
    "        else:\n",
    "            return \"unknown\"\n",
    "    except:\n",
    "        return \"unknown\"  \n",
    "\n",
    "temp = trans_df['deep_translated_text'].apply(func)\n",
    "\n",
    "print(\"Languages found \",temp.unique())\n",
    "print(\"Total instances where it was not english\",temp[temp != 'en'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Since we have not reliable way to detect languages we still managed to half the number of instances and adding this step in our pipline we definitely help "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting categorical values into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded dataset saved: ../Data/Pre-processed/encoded_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../Data/Pre-processed/translated_output.csv')\n",
    "\n",
    "category_encoder = OrdinalEncoder()\n",
    "df[\"category\"] = category_encoder.fit_transform(df[[\"category\"]])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"label\"] = label_encoder.fit_transform(df[\"label\"])\n",
    "\n",
    "output_path = \"../Data/Pre-processed/encoded_dataset.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Encoded dataset saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing Pipeline\n",
    "\n",
    "1. Library Initialization\n",
    "- Inflect Engine: Converts numbers to words (e.g., \"4\" → \"four\").\n",
    "- Lemmatizer (WordNetLemmatizer): Reduces words to their base form (e.g., \"running\" → \"run\").\n",
    "- Stemmer (SnowballStemmer): Reduces words to their root (e.g., \"running\" → \"run\"), but more aggressively than lemmatization.\n",
    "- Stopwords Set: Contains common words (e.g., \"the\", \"is\") to be optionally removed.\n",
    "- SymSpell: Used for spelling correction with a predefined dictionary.\n",
    "\n",
    "2. Utility Functions\n",
    "`remove_emojis(text)`\n",
    "- Removes non-ASCII characters (including most emojis).\n",
    "\n",
    "`convert_numbers(text)`\n",
    "- Converts numbers into words using the `inflect` library.\n",
    "\n",
    "`correct_spelling(word)`\n",
    "- Uses SymSpell to find the closest correct spelling for a word.\n",
    "\n",
    "`get_wordnet_pos(word)`\n",
    "- Maps POS (Part of Speech) tags from NLTK to WordNet for more accurate lemmatization.\n",
    "\n",
    "3. Advanced Text Preprocessing Function\n",
    "`advanced_preprocess_text(text, remove_stopwords, use_stemming, use_lemmatization, use_spell_correction, expand_contractions_flag)`\n",
    "- Removes emojis and expands contractions (e.g., \"don't\" → \"do not\").\n",
    "- Removes HTML tags.\n",
    "- Converts text to lowercase.\n",
    "- Converts numbers to words.\n",
    "- Removes punctuation for better tokenization.\n",
    "- Tokenizes the text into words.\n",
    "- Applies optional transformations:\n",
    "  - Spell correction (if enabled).\n",
    "  - Stopword removal (if enabled).\n",
    "  - Stemming or Lemmatization (default: Lemmatization).\n",
    "\n",
    "4. Batch Preprocessing for a DataFrame\n",
    "`advanced_preprocess_texts(df, remove_stopwords, use_stemming, use_lemmatization, use_spell_correction, expand_contractions_flag, filename)`\n",
    "- Applies `advanced_preprocess_text()` to the `deep_translated_text` column of a DataFrame.\n",
    "- Saves the processed text to a CSV file in `../Data/Pre-processed/`.\n",
    "\n",
    "5. Running Preprocessing with Different Configurations\n",
    "- Loads dataset from `encoded_dataset.csv`.\n",
    "- Defines five configurations with different combinations of:\n",
    "  - Stopword removal.\n",
    "  - Lemmatization or stemming.\n",
    "  - Spell correction.\n",
    "  - Contraction expansion.\n",
    "- Processes dataset multiple times using these configurations.\n",
    "- Saves each processed version as a separate CSV file.\n",
    "\n",
    "6. Execution\n",
    "- Runs preprocessing for all configurations.\n",
    "- Saves preprocessed datasets.\n",
    "- Prints completion message.\n",
    "\n",
    "This setup enables efficient text preprocessing while experimenting with different NLP techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file saved: ../Data/Pre-processed/preprocessed_lemmatization.csv\n",
      "Processed file saved: ../Data/Pre-processed/preprocessed_no_stopwords_no_lemmatization.csv\n",
      "Processed file saved: ../Data/Pre-processed/preprocessed_no_stopwords.csv\n",
      "Processed file saved: ../Data/Pre-processed/preprocessed_stemming_no_stopwords.csv\n",
      "Processed file saved: ../Data/Pre-processed/preprocessed_stemming.csv\n",
      "All preprocessed datasets saved.\n"
     ]
    }
   ],
   "source": [
    "inflect_engine = inflect.engine()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = \"../Assets/frequency_dictionary_en_82_765.txt\"\n",
    "if not sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1):\n",
    "    raise FileNotFoundError(f\"SymSpell dictionary file not found at {dictionary_path}\")\n",
    "\n",
    "def remove_emojis(text: str) -> str:\n",
    "    \"\"\"Remove all non-ASCII characters (including most emojis).\"\"\"\n",
    "    return re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "\n",
    "def convert_numbers(text: str) -> str:\n",
    "    \"\"\"Convert numeric digits to their word representation (e.g., '4' -> 'four').\"\"\"\n",
    "    return re.sub(r'\\b\\d+\\b', lambda x: inflect_engine.number_to_words(x.group()), text)\n",
    "\n",
    "def correct_spelling(word: str) -> str:\n",
    "    \"\"\"Correct spelling using SymSpell, returning the closest suggestion if available.\"\"\"\n",
    "    suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "    return suggestions[0].term if suggestions else word\n",
    "\n",
    "def get_wordnet_pos(word: str):\n",
    "    \"\"\"Map NLTK POS tags to WordNet POS for better lemmatization.\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def advanced_preprocess_text(\n",
    "    text: str,\n",
    "    remove_stopwords: bool = False,\n",
    "    use_stemming: bool = False,\n",
    "    use_lemmatization: bool = True,\n",
    "    use_spell_correction: bool = False,\n",
    "    expand_contractions_flag: bool = True\n",
    ") -> str:\n",
    "    \"\"\"Preprocess text by removing emojis, expanding contractions, converting numbers, etc.\"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = remove_emojis(text)\n",
    "    if expand_contractions_flag:\n",
    "        text = contractions.fix(text)\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = text.lower()\n",
    "    text = convert_numbers(text)\n",
    "\n",
    "    text_clean = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    words = word_tokenize(text_clean)\n",
    "\n",
    "    if use_spell_correction:\n",
    "        words = [correct_spelling(word) for word in words]\n",
    "\n",
    "    if remove_stopwords:\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    if use_stemming:\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "    elif use_lemmatization:\n",
    "        words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "def advanced_preprocess_texts(\n",
    "    df: pd.DataFrame,\n",
    "    remove_stopwords: bool = False,\n",
    "    use_stemming: bool = False,\n",
    "    use_lemmatization: bool = True,\n",
    "    use_spell_correction: bool = False,\n",
    "    expand_contractions_flag: bool = True,\n",
    "    filename: str = \"processed_output\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Apply the advanced preprocessing pipeline to the 'deep_translated_text' column of the DataFrame.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"processed_text\"] = df[\"deep_translated_text\"].apply(\n",
    "        lambda x: advanced_preprocess_text(\n",
    "            x,\n",
    "            remove_stopwords,\n",
    "            use_stemming,\n",
    "            use_lemmatization,\n",
    "            use_spell_correction,\n",
    "            expand_contractions_flag\n",
    "        )\n",
    "    )\n",
    "    os.makedirs(\"../Data/Pre-processed/\", exist_ok=True)\n",
    "    filepath = f\"../Data/Pre-processed/{filename}.csv\"\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"Processed file saved: {filepath}\")\n",
    "    return df\n",
    "\n",
    "df = pd.read_csv(\"../Data/Pre-processed/encoded_dataset.csv\")\n",
    "\n",
    "configurations = [\n",
    "    {\n",
    "        \"remove_stopwords\": False,\n",
    "        \"use_stemming\": False,\n",
    "        \"use_lemmatization\": True,\n",
    "        \"use_spell_correction\": False,\n",
    "        \"expand_contractions_flag\": True,\n",
    "        \"filename\": \"preprocessed_lemmatization\"\n",
    "    },\n",
    "    {\n",
    "        \"remove_stopwords\": True,\n",
    "        \"use_stemming\": False,\n",
    "        \"use_lemmatization\": False,\n",
    "        \"use_spell_correction\": False,\n",
    "        \"expand_contractions_flag\": True,\n",
    "        \"filename\": \"preprocessed_no_stopwords_no_lemmatization\"\n",
    "    },\n",
    "    {\n",
    "        \"remove_stopwords\": True,\n",
    "        \"use_stemming\": False,\n",
    "        \"use_lemmatization\": True,\n",
    "        \"use_spell_correction\": False,\n",
    "        \"expand_contractions_flag\": True,\n",
    "        \"filename\": \"preprocessed_no_stopwords\"\n",
    "    },\n",
    "    {\n",
    "        \"remove_stopwords\": True,\n",
    "        \"use_stemming\": True,\n",
    "        \"use_lemmatization\": False,\n",
    "        \"use_spell_correction\": False,\n",
    "        \"expand_contractions_flag\": True,\n",
    "        \"filename\": \"preprocessed_stemming_no_stopwords\"\n",
    "    },\n",
    "    {\n",
    "        \"remove_stopwords\": False,\n",
    "        \"use_stemming\": True,\n",
    "        \"use_lemmatization\": False,\n",
    "        \"use_spell_correction\": False,\n",
    "        \"expand_contractions_flag\": True,\n",
    "        \"filename\": \"preprocessed_stemming\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for config in configurations:\n",
    "    advanced_preprocess_texts(\n",
    "        df,\n",
    "        remove_stopwords=config[\"remove_stopwords\"],\n",
    "        use_stemming=config[\"use_stemming\"],\n",
    "        use_lemmatization=config[\"use_lemmatization\"],\n",
    "        use_spell_correction=config[\"use_spell_correction\"],\n",
    "        expand_contractions_flag=config[\"expand_contractions_flag\"],\n",
    "        filename=config[\"filename\"]\n",
    "    )\n",
    "\n",
    "print(\"All preprocessed datasets saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
