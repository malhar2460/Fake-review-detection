{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"pip install -q dagshub mlflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not delete run 2d0257df3f4f44ec8e7115bf59e61c85: API request to endpoint /api/2.0/mlflow/runs/delete failed with error code 403 != 200. Response body: ''\n",
      "Could not delete run 51a8715a37c444a19ef3a9fc0018c6b0: API request to endpoint /api/2.0/mlflow/runs/delete failed with error code 403 != 200. Response body: ''\n",
      "Could not delete run 4a870c3ae9d646909c750b4fd09c184a: API request to endpoint /api/2.0/mlflow/runs/delete failed with error code 403 != 200. Response body: ''\n",
      "Could not delete run 3f12b79a5b6c46d1acb31d5b271370ab: API request to endpoint /api/2.0/mlflow/runs/delete failed with error code 403 != 200. Response body: ''\n",
      "Could not delete run 9f603c8dd2874928b176cb3afcede509: API request to endpoint /api/2.0/mlflow/runs/delete failed with error code 403 != 200. Response body: ''\n",
      "Could not delete run 94877f52e89442e0b16c0ace287ee091: API request to endpoint /api/2.0/mlflow/runs/delete failed with error code 403 != 200. Response body: ''\n",
      "Could not delete run db45689c2a8c4640aea50303e42b27c7: API request to endpoint /api/2.0/mlflow/runs/delete failed with error code 403 != 200. Response body: ''\n",
      "Could not delete run 1f2394fc7c8a405aba1cd27601fb54e3: API request to endpoint /api/2.0/mlflow/runs/delete failed with error code 403 != 200. Response body: ''\n",
      "Could not delete run 46612e028ebc4525b95d0622fef63dae: API request to endpoint /api/2.0/mlflow/runs/delete failed with error code 403 != 200. Response body: ''\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "mlflow.set_tracking_uri(\"https://dagshub.com/malhar.c.prajapati/my-first-repo.mlflow\")\n",
    "client = MlflowClient()\n",
    "default_experiment = client.get_experiment_by_name(\"Fake Review Detection\")\n",
    "runs = client.search_runs(experiment_ids=[default_experiment.experiment_id])\n",
    "for run in runs:\n",
    "    try:\n",
    "        client.delete_run(run.info.run_id)\n",
    "        print(f\"Deleted run {run.info.run_id} from experiment {default_experiment.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not delete run {run.info.run_id}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/06 21:36:12 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run preprocessed_lemmatization_features.csv_Tfidf_LogisticRegression at: https://dagshub.com/malhar.c.prajapati/my-first-repo.mlflow/#/experiments/1/runs/4a870c3ae9d646909c750b4fd09c184a\n",
      "üß™ View experiment at: https://dagshub.com/malhar.c.prajapati/my-first-repo.mlflow/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/06 22:02:33 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run preprocessed_lemmatization_features.csv_Tfidf_RandomForest at: https://dagshub.com/malhar.c.prajapati/my-first-repo.mlflow/#/experiments/1/runs/51a8715a37c444a19ef3a9fc0018c6b0\n",
      "üß™ View experiment at: https://dagshub.com/malhar.c.prajapati/my-first-repo.mlflow/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.keras\n",
    "import mlflow.data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = os.getenv(\"MLFLOW_TRACKING_USERNAME\", \"malhar.c.prajapati\")\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = os.getenv(\"MLFLOW_TRACKING_PASSWORD\", \"f222587ea4fa84ee148e478d207d3112535c5edd\")\n",
    "mlflow.set_tracking_uri(\"https://dagshub.com/malhar.c.prajapati/my-first-repo.mlflow\")\n",
    "mlflow.set_experiment(\"Fake Review Detection\")\n",
    "\n",
    "feature_files = [\n",
    "    \"../Data/Feature-Engineered/preprocessed_lemmatization_features.csv\",\n",
    "    \"../Data/Feature-Engineered/preprocessed_no_stopwords_features.csv\",\n",
    "    \"../Data/Feature-Engineered/preprocessed_no_stopwords_no_lemmatization_features.csv\",\n",
    "    \"../Data/Feature-Engineered/preprocessed_stemming_features.csv\",\n",
    "    \"../Data/Feature-Engineered/preprocessed_stemming_no_stopwords_features.csv\"\n",
    "]\n",
    "\n",
    "embedding_files = [\n",
    "    \"../../embeddings/preprocessed_lemmatization_bert.csv\",\n",
    "    \"../../embeddings/preprocessed_lemmatization_glove.csv\",\n",
    "    \"../../embeddings/preprocessed_lemmatization_tfidf.csv\",\n",
    "    \"../../embeddings/preprocessed_no_stopwords_bert.csv\",\n",
    "    \"../../embeddings/preprocessed_no_stopwords_glove.csv\",\n",
    "    \"../../embeddings/preprocessed_no_stopwords_no_lemmatization_bert.csv\",\n",
    "    \"../../embeddings/preprocessed_no_stopwords_no_lemmatization_glove.csv\",\n",
    "    \"../../embeddings/preprocessed_no_stopwords_no_lemmatization_tfidf.csv\",\n",
    "    \"../../embeddings/preprocessed_no_stopwords_tfidf.csv\",\n",
    "    \"../../embeddings/preprocessed_stemming_bert.csv\",\n",
    "    \"../../embeddings/preprocessed_stemming_glove.csv\",\n",
    "    \"../../embeddings/preprocessed_stemming_no_stopwords_bert.csv\",\n",
    "    \"../../embeddings/preprocessed_stemming_no_stopwords_glove.csv\",\n",
    "    \"../../embeddings/preprocessed_stemming_no_stopwords_tfidf.csv\",\n",
    "    \"../../embeddings/preprocessed_stemming_tfidf.csv\"\n",
    "]\n",
    "\n",
    "files = feature_files + embedding_files\n",
    "\n",
    "models = {\n",
    "    \"LogisticRegression\": (\n",
    "        LogisticRegression,\n",
    "        {\"C\": [0.1, 1], \"solver\": [\"liblinear\"], \"max_iter\": [100]}\n",
    "    ),\n",
    "    \"RandomForest\": (\n",
    "        RandomForestClassifier,\n",
    "        {\"n_estimators\": [50, 100], \"max_depth\": [None, 10]}\n",
    "    ),\n",
    "    \"SVC\": (\n",
    "        SVC,\n",
    "        {\"C\": [0.1, 1], \"kernel\": [\"linear\"]}\n",
    "    )\n",
    "}\n",
    "\n",
    "progress_file = \"progress_log.csv\"\n",
    "if os.path.exists(progress_file):\n",
    "    dfp = pd.read_csv(progress_file)\n",
    "    processed_keys = set(dfp[\"run_key\"].tolist())\n",
    "else:\n",
    "    processed_keys = set()\n",
    "\n",
    "def log_cm(y_true, y_pred, run_key, prefix):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    cm_path = f\"../Reports/confusion_matrix_{prefix}_{run_key}.png\"\n",
    "    plt.savefig(cm_path)\n",
    "    mlflow.log_artifact(cm_path)\n",
    "    plt.close()\n",
    "\n",
    "for f in files:\n",
    "    if not os.path.exists(f):\n",
    "        continue\n",
    "    df = pd.read_csv(f)\n",
    "    if \"label\" not in df.columns:\n",
    "        continue\n",
    "    y = df[\"label\"].values\n",
    "    if y.dtype == object or y.dtype == \"O\":\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "    X = df.drop(columns=[\"label\"]).values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    name_prefix = os.path.basename(f)\n",
    "    for m_name, (Cls, param_grid) in models.items():\n",
    "        run_key = name_prefix + \"_\" + m_name\n",
    "        if run_key in processed_keys:\n",
    "            continue\n",
    "        with mlflow.start_run(run_name=run_key):\n",
    "            mlflow.log_param(\"file_name\", f)\n",
    "            mlflow.log_param(\"model_type\", m_name)\n",
    "            try:\n",
    "                ds = mlflow.data.from_pandas(df, source=f)\n",
    "                mlflow.data.log_dataset(ds, name=\"embedding_data\")\n",
    "            except Exception:\n",
    "                mlflow.log_artifact(f, artifact_path=\"dataset_csv\")\n",
    "            gs = GridSearchCV(Cls(), param_grid, cv=3, scoring=\"accuracy\", n_jobs=1)\n",
    "            gs.fit(X_train, y_train)\n",
    "            best_model = gs.best_estimator_\n",
    "            p = best_model.predict(X_test)\n",
    "            a = accuracy_score(y_test, p)\n",
    "            pr = precision_score(y_test, p, average=\"weighted\")\n",
    "            r = recall_score(y_test, p, average=\"weighted\")\n",
    "            f1 = f1_score(y_test, p, average=\"weighted\")\n",
    "            mlflow.log_params(gs.best_params_)\n",
    "            mlflow.log_metric(\"accuracy\", a)\n",
    "            mlflow.log_metric(\"precision\", pr)\n",
    "            mlflow.log_metric(\"recall\", r)\n",
    "            mlflow.log_metric(\"f1_score\", f1)\n",
    "            mlflow.sklearn.log_model(best_model, m_name + \"_Model\")\n",
    "            log_cm(y_test, p, run_key, \"ML\")\n",
    "        mlflow.end_run()\n",
    "        new_row = [run_key, f, m_name, a, pr, r, f1]\n",
    "        dfp_new = pd.DataFrame([new_row], columns=[\"run_key\", \"File\", \"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
    "        if os.path.exists(progress_file):\n",
    "            dfp_new.to_csv(progress_file, mode='a', index=False, header=False)\n",
    "        else:\n",
    "            dfp_new.to_csv(progress_file, index=False)\n",
    "        processed_keys.add(run_key)\n",
    "    dl_run_key = \"DL_2LSTM_\" + name_prefix\n",
    "    if dl_run_key in processed_keys:\n",
    "        continue\n",
    "    num_classes = len(np.unique(y))\n",
    "    with mlflow.start_run(run_name=dl_run_key):\n",
    "        mlflow.log_param(\"file_name\", f)\n",
    "        mlflow.log_param(\"model_type\", \"2-Layer LSTM\")\n",
    "        try:\n",
    "            ds = mlflow.data.from_pandas(df, source=f)\n",
    "            mlflow.data.log_dataset(ds, name=\"embedding_data\")\n",
    "        except Exception:\n",
    "            mlflow.log_artifact(f, artifact_path=\"dataset_csv\")\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(input_dim=X.shape[1], output_dim=128, input_length=X.shape[1]))\n",
    "        model.add(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\n",
    "        model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "        model.add(Dense(64, activation=\"relu\"))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "        model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "        model.fit(X_train, y_train, epochs=3, batch_size=32, validation_split=0.1, verbose=0)\n",
    "        loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "        preds_prob = model.predict(X_test)\n",
    "        preds = preds_prob.argmax(axis=1)\n",
    "        prec = precision_score(y_test, preds, average=\"weighted\")\n",
    "        rec = recall_score(y_test, preds, average=\"weighted\")\n",
    "        f1 = f1_score(y_test, preds, average=\"weighted\")\n",
    "        mlflow.log_metric(\"accuracy\", acc)\n",
    "        mlflow.log_metric(\"precision\", prec)\n",
    "        mlflow.log_metric(\"recall\", rec)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        mlflow.keras.log_model(model, \"2LSTM_Model\")\n",
    "        log_cm(y_test, preds, dl_run_key, \"DL\")\n",
    "    mlflow.end_run()\n",
    "    new_row = [dl_run_key, f, \"2-Layer LSTM\", acc, prec, rec, f1]\n",
    "    dfp_new = pd.DataFrame([new_row], columns=[\"run_key\", \"File\", \"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
    "    if os.path.exists(progress_file):\n",
    "        dfp_new.to_csv(progress_file, mode='a', index=False, header=False)\n",
    "    else:\n",
    "        dfp_new.to_csv(progress_file, index=False)\n",
    "    processed_keys.add(dl_run_key)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
