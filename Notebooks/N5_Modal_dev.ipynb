{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"pip install -q dagshub mlflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted run 6d3343057aeb461bbd4e588313695b7c from experiment Fake Review Detection\n",
      "Deleted run d05358688fa1474fb63268ec3af06e71 from experiment Fake Review Detection\n",
      "Deleted run 3799d9457af24687b9f68241b98a305a from experiment Fake Review Detection\n",
      "Deleted run 3801243913754f3b9a1d19a332d7427b from experiment Fake Review Detection\n",
      "Deleted run ce8a028067434959ae59a4feb5d9aa6a from experiment Fake Review Detection\n",
      "Deleted run cf7f9905b5964c21827316a7b3bb8d72 from experiment Fake Review Detection\n",
      "Deleted run 4b2f4144b7664c7aaab9464ca92b3388 from experiment Fake Review Detection\n",
      "Deleted run 639419fbeb3842719dbeb69b7c199836 from experiment Fake Review Detection\n",
      "Deleted run 226f194a2b174c28a33d239dba633e16 from experiment Fake Review Detection\n",
      "Deleted run 57785dd395974d68ab6333370457dc0d from experiment Fake Review Detection\n",
      "Deleted run 0b375ef87b7b476d926d6362bce9c9cb from experiment Fake Review Detection\n",
      "Deleted run bdefd47c844d4d66b7f7d56e802a7878 from experiment Fake Review Detection\n",
      "Deleted run f1fb9ea3172246feadc2e2272611a824 from experiment Fake Review Detection\n",
      "Deleted run 64b0edb6ac9a47d5a5fe8932fa7c243d from experiment Fake Review Detection\n",
      "Deleted run 75202df2520742429ab36e5ecadd211a from experiment Fake Review Detection\n",
      "Deleted run ce7ea51303d74638978c3f51992c76a2 from experiment Fake Review Detection\n",
      "Deleted run 638dfe9ef5bd46ac845c141961d6c840 from experiment Fake Review Detection\n",
      "Deleted run 332c869c138d48b9b1cc87f008f185e0 from experiment Fake Review Detection\n",
      "Deleted run ded874114733470db545df093fe7af1c from experiment Fake Review Detection\n",
      "Deleted run 40c4cced0aab4197b361e5bdf8361ab9 from experiment Fake Review Detection\n",
      "Deleted run 12454ea7406d4dd2b98d582314572d04 from experiment Fake Review Detection\n",
      "Deleted run a9cf3058495642298901d082fff948aa from experiment Fake Review Detection\n",
      "Deleted run ec666f2e553b43b0abaa4bd7981da36b from experiment Fake Review Detection\n",
      "Deleted run 4a5af24ac01b4da18d42d78b4cdeb9cc from experiment Fake Review Detection\n",
      "Deleted run 14eb09a1e89c49428336f156cd5fdd55 from experiment Fake Review Detection\n",
      "Deleted run d0221ebb220247e684ee4630ab06b133 from experiment Fake Review Detection\n",
      "Deleted run 05608e881d394141be1924f1ef86a6ec from experiment Fake Review Detection\n",
      "Deleted run 363d87aa2e5f4a718a5888c1b78b34dc from experiment Fake Review Detection\n",
      "Deleted run c5db4a8b70974630b0f738ca03fd5112 from experiment Fake Review Detection\n",
      "Deleted run bbd1019db1cd4bb2895ef57afb03f85f from experiment Fake Review Detection\n",
      "Deleted run 17e5dae8b1024db9a5d1c37860bc1aff from experiment Fake Review Detection\n",
      "Deleted run 267b2ee72e85471f81ac6151320fb698 from experiment Fake Review Detection\n",
      "Deleted run 6915ff4bd34647c7a7665eb599d407c9 from experiment Fake Review Detection\n",
      "Deleted run 99ea699fb48b474380c47aae6f2d276b from experiment Fake Review Detection\n",
      "Deleted run 72a230aef0fd4bd8b8121ae429eb3d7c from experiment Fake Review Detection\n",
      "Deleted run 6140afd578744f4d9ccdda65d222d549 from experiment Fake Review Detection\n",
      "Deleted run 34936a509219406bb9b353b00677068f from experiment Fake Review Detection\n",
      "Deleted run 02fb8b10710f44779c012ed84bc6a918 from experiment Fake Review Detection\n",
      "Deleted run ecd14352fbd24edb841eb5d7319513d3 from experiment Fake Review Detection\n",
      "Deleted run f08ccbfe14c04079907828fc8e1f5e39 from experiment Fake Review Detection\n",
      "Deleted run 42afa48d533a48cd840b84a7a30391f1 from experiment Fake Review Detection\n",
      "Deleted run 8f13089a755c40ab8c889a355735d1bd from experiment Fake Review Detection\n",
      "Deleted run 48f1677143c2401f8d326ef421c2be7f from experiment Fake Review Detection\n",
      "Deleted run bc1f2cf065364bda86a5ac06bd8541de from experiment Fake Review Detection\n",
      "Deleted run 537d47614b7b4f9099ea543fb0fab294 from experiment Fake Review Detection\n",
      "Deleted run 3280f5cf8a2a4b0f8279dcb63aa99c5a from experiment Fake Review Detection\n",
      "Deleted run e4693ab3353b4d948e89da3f93694238 from experiment Fake Review Detection\n",
      "Deleted run 53182e8c91b44f579af5ada450fdf839 from experiment Fake Review Detection\n",
      "Deleted run 04e76cd757754bf4bb0e5336e2ca6c6b from experiment Fake Review Detection\n",
      "Deleted run de7f19f289ff416b93647a752d9e650e from experiment Fake Review Detection\n",
      "Deleted run d5fd4a37f6a44caa8ac3e68d2c2d0624 from experiment Fake Review Detection\n",
      "Deleted run 4400c51d1d45483d945447d52851ca7f from experiment Fake Review Detection\n",
      "Deleted run 35af50571e7445ed8ead063efb0e959d from experiment Fake Review Detection\n",
      "Deleted run 9f211e1ea7de4eec88577fcecf77afff from experiment Fake Review Detection\n",
      "Deleted run 099d5c64ffe94cb39f3b1ae7e2f281d4 from experiment Fake Review Detection\n",
      "Deleted run e0b653bb51e649afb4d4bf7a9bd36721 from experiment Fake Review Detection\n",
      "Deleted run 96acb386253d4abf901e2e81e92cb755 from experiment Fake Review Detection\n",
      "Deleted run c1ba74fb54c748c8a21bc63f57f1a4ad from experiment Fake Review Detection\n",
      "Deleted run 2ddf08ceb4f54fc8bd3796be6fc3547b from experiment Fake Review Detection\n",
      "Deleted run 2d0257df3f4f44ec8e7115bf59e61c85 from experiment Fake Review Detection\n",
      "Deleted run 51a8715a37c444a19ef3a9fc0018c6b0 from experiment Fake Review Detection\n",
      "Deleted run 4a870c3ae9d646909c750b4fd09c184a from experiment Fake Review Detection\n",
      "Deleted run 3f12b79a5b6c46d1acb31d5b271370ab from experiment Fake Review Detection\n",
      "Deleted run 9f603c8dd2874928b176cb3afcede509 from experiment Fake Review Detection\n",
      "Deleted run 94877f52e89442e0b16c0ace287ee091 from experiment Fake Review Detection\n",
      "Deleted run db45689c2a8c4640aea50303e42b27c7 from experiment Fake Review Detection\n",
      "Deleted run 1f2394fc7c8a405aba1cd27601fb54e3 from experiment Fake Review Detection\n",
      "Deleted run 46612e028ebc4525b95d0622fef63dae from experiment Fake Review Detection\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Set tracking URI\n",
    "mlflow.set_tracking_uri(\"https://dagshub.com/malhar.c.prajapati/my-first-repo.mlflow\")\n",
    "\n",
    "# Authenticate using environment variables\n",
    "mlflow.set_tracking_uri(\"https://malhar.c.prajapati:f222587ea4fa84ee148e478d207d3112535c5edd@dagshub.com/malhar.c.prajapati/my-first-repo.mlflow\")\n",
    "\n",
    "client = MlflowClient()\n",
    "default_experiment = client.get_experiment_by_name(\"Fake Review Detection\")\n",
    "\n",
    "runs = client.search_runs(experiment_ids=[default_experiment.experiment_id])\n",
    "\n",
    "for run in runs:\n",
    "    try:\n",
    "        client.delete_run(run.info.run_id)\n",
    "        print(f\"Deleted run {run.info.run_id} from experiment {default_experiment.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not delete run {run.info.run_id}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConvergenceWarning\n\u001b[0;32m      4\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, category\u001b[38;5;241m=\u001b[39mConvergenceWarning)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmlflow\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MALHAR PRAJAPATI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\__init__.py:73\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401 E402\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     __check_build,\n\u001b[0;32m     71\u001b[0m     _distributor_init,\n\u001b[0;32m     72\u001b[0m )\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     76\u001b[0m _submodules \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompose\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    115\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\MALHAR PRAJAPATI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "File \u001b[1;32mc:\\Users\\MALHAR PRAJAPATI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\__init__.py:15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _joblib, metadata_routing\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_chunking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MALHAR PRAJAPATI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_chunking.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[0;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MALHAR PRAJAPATI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumbers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Integral, Real\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n",
      "File \u001b[1;32mc:\\Users\\MALHAR PRAJAPATI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\sparse\\__init__.py:308\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_matrix_io\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# For backward compatibility with v0.19.\u001b[39;00m\n\u001b[1;32m--> 308\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m csgraph\n\u001b[0;32m    310\u001b[0m \u001b[38;5;66;03m# Deprecated namespaces, to be removed in v2.0.0\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    312\u001b[0m     base, bsr, compressed, construct, coo, csc, csr, data, dia, dok, extract,\n\u001b[0;32m    313\u001b[0m     lil, sparsetools, sputils\n\u001b[0;32m    314\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\MALHAR PRAJAPATI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\sparse\\csgraph\\__init__.py:185\u001b[0m\n\u001b[0;32m    157\u001b[0m __docformat__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrestructuredtext en\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconnected_components\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    160\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlaplacian\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    161\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshortest_path\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsgraph_to_masked\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    183\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNegativeCycleError\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 185\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_laplacian\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m laplacian\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_shortest_path\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    187\u001b[0m     shortest_path, floyd_warshall, dijkstra, bellman_ford, johnson,\n\u001b[0;32m    188\u001b[0m     NegativeCycleError\n\u001b[0;32m    189\u001b[0m )\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_traversal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    191\u001b[0m     breadth_first_order, depth_first_order, breadth_first_tree,\n\u001b[0;32m    192\u001b[0m     depth_first_tree, connected_components\n\u001b[0;32m    193\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\MALHAR PRAJAPATI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\sparse\\csgraph\\_laplacian.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m issparse\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LinearOperator\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sputils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convert_pydata_sparse_to_scipy, is_pydata_spmatrix\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m###############################################################################\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Graph laplacian\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MALHAR PRAJAPATI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\sparse\\linalg\\__init__.py:129\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mSparse linear algebra (:mod:`scipy.sparse.linalg`)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m==================================================\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    126\u001b[0m \n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_isolve\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dsolve\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_interface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\MALHAR PRAJAPATI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\sparse\\linalg\\_isolve\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterative Solvers for Sparse Linear Systems\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#from info import __doc__\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miterative\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mminres\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m minres\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlgmres\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lgmres\n",
      "File \u001b[1;32mc:\\Users\\MALHAR PRAJAPATI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\sparse\\linalg\\_isolve\\iterative.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_interface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LinearOperator\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_system\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _NoValue, _deprecate_positional_args\n\u001b[0;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbicg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbicgstab\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcgs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgmres\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqmr\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\MALHAR PRAJAPATI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\linalg\\__init__.py:215\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decomp_schur\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decomp_polar\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m--> 215\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_matfuncs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlapack\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\MALHAR PRAJAPATI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\linalg\\_matfuncs.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_expm_frechet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m expm_frechet, expm_cond\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_matfuncs_sqrtm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sqrtm\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_matfuncs_expm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pick_pade_structure, pade_UV_calc\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# deprecated imports to be removed in SciPy 1.13.0\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m single  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32mscipy\\\\linalg\\\\_matfuncs_expm.pyx:1\u001b[0m, in \u001b[0;36minit scipy.linalg._matfuncs_expm\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:645\u001b[0m, in \u001b[0;36mparent\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.keras\n",
    "import mlflow.data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "\n",
    "# -------------------------------\n",
    "# Experiment Setup\n",
    "# -------------------------------\n",
    "EXPERIMENT_NAME = \"Fake Review Detection 2.0\"\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "try:\n",
    "    experiment = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    if experiment is None:\n",
    "        experiment_id = client.create_experiment(EXPERIMENT_NAME)\n",
    "    else:\n",
    "        experiment_id = experiment.experiment_id\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "except Exception as e:\n",
    "    # If creation fails (403 error), display error and fallback to default experiment.\n",
    "    print(f\"Could not set or create experiment {EXPERIMENT_NAME}: {e}\")\n",
    "    mlflow.set_experiment(\"Default\")\n",
    "\n",
    "# -------------------------------\n",
    "# Define file lists\n",
    "# -------------------------------\n",
    "feature_files = [\n",
    "    \"../Data/Feature-Engineered/preprocessed_lemmatization_features.csv\",\n",
    "    \"../Data/Feature-Engineered/preprocessed_no_stopwords_features.csv\",\n",
    "    \"../Data/Feature-Engineered/preprocessed_no_stopwords_no_lemmatization_features.csv\",\n",
    "    \"../Data/Feature-Engineered/preprocessed_stemming_features.csv\",\n",
    "    \"../Data/Feature-Engineered/preprocessed_stemming_no_stopwords_features.csv\"\n",
    "]\n",
    "embedding_files = [\n",
    "    \"../../embeddings/preprocessed_lemmatization_bert.csv\",\n",
    "    \"../../embeddings/preprocessed_lemmatization_glove.csv\",\n",
    "    \"../../embeddings/preprocessed_lemmatization_tfidf.csv\",\n",
    "    \"../../embeddings/preprocessed_no_stopwords_bert.csv\",\n",
    "    \"../../embeddings/preprocessed_no_stopwords_glove.csv\",\n",
    "    \"../../embeddings/preprocessed_no_stopwords_no_lemmatization_bert.csv\",\n",
    "    \"../../embeddings/preprocessed_no_stopwords_no_lemmatization_glove.csv\",\n",
    "    \"../../embeddings/preprocessed_no_stopwords_no_lemmatization_tfidf.csv\",\n",
    "    \"../../embeddings/preprocessed_no_stopwords_tfidf.csv\",\n",
    "    \"../../embeddings/preprocessed_stemming_bert.csv\",\n",
    "    \"../../embeddings/preprocessed_stemming_glove.csv\",\n",
    "    \"../../embeddings/preprocessed_stemming_no_stopwords_bert.csv\",\n",
    "    \"../../embeddings/preprocessed_stemming_no_stopwords_glove.csv\",\n",
    "    \"../../embeddings/preprocessed_stemming_no_stopwords_tfidf.csv\",\n",
    "    \"../../embeddings/preprocessed_stemming_tfidf.csv\"\n",
    "]\n",
    "files = feature_files + embedding_files\n",
    "\n",
    "# -------------------------------\n",
    "# Define ML models and parameter grids\n",
    "# -------------------------------\n",
    "models = {\n",
    "    \"LogisticRegression\": (LogisticRegression, {\"C\": [0.1, 1], \"solver\": [\"liblinear\"], \"max_iter\": [100]}),\n",
    "    \"RandomForest\": (RandomForestClassifier, {\"n_estimators\": [50, 100], \"max_depth\": [None, 10]}),\n",
    "    \"SVC\": (SVC, {\"C\": [0.1, 1], \"kernel\": [\"linear\"]})\n",
    "}\n",
    "\n",
    "progress_file = \"progress_log.csv\"\n",
    "if os.path.exists(progress_file):\n",
    "    dfp = pd.read_csv(progress_file)\n",
    "    processed_keys = set(dfp[\"run_key\"].tolist())\n",
    "else:\n",
    "    processed_keys = set()\n",
    "\n",
    "def log_confusion_matrix(y_true, y_pred, run_key, prefix):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    cm_path = f\"../Reports/confusion_matrix_{prefix}_{run_key}.png\"\n",
    "    plt.savefig(cm_path)\n",
    "    mlflow.log_artifact(cm_path)\n",
    "    plt.close()\n",
    "\n",
    "def log_dataset(df, source_file):\n",
    "    try:\n",
    "        ds = mlflow.data.from_pandas(df, source=source_file)\n",
    "        mlflow.data.log_dataset(ds, name=\"embedding_data\")\n",
    "    except Exception:\n",
    "        mlflow.log_artifact(source_file, artifact_path=\"dataset_csv\")\n",
    "\n",
    "# -------------------------------\n",
    "# Traditional ML Experiments (applied to all files)\n",
    "# -------------------------------\n",
    "for f in files:\n",
    "    if not os.path.exists(f):\n",
    "        continue\n",
    "    df = pd.read_csv(f)\n",
    "    if \"label\" not in df.columns:\n",
    "        continue\n",
    "    df.dropna(inplace=True)\n",
    "    y = df[\"label\"].values\n",
    "    if y.dtype == object or y.dtype == \"O\":\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "    # For feature files with computed metrics, use numeric columns; otherwise use all columns except label.\n",
    "    if f in feature_files and \"processed_text\" in df.columns and \"lexical_diversity\" in df.columns:\n",
    "        numeric_cols = [\"lexical_diversity\", \"avg_word_length\", \"sentiment_polarity\",\n",
    "                        \"subjectivity\", \"flesch_reading_ease\", \"sentence_length\",\n",
    "                        \"named_entity_count\", \"noun_count\", \"verb_count\", \"adj_count\", \"adv_count\"]\n",
    "        available_cols = [col for col in numeric_cols if col in df.columns]\n",
    "        X = df[available_cols].values\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    else:\n",
    "        X = df.drop(columns=[\"label\"]).values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    name_prefix = os.path.basename(f)\n",
    "    for m_name, (Cls, param_grid) in models.items():\n",
    "        run_key = f\"{name_prefix}_{m_name}_ML\"\n",
    "        if run_key in processed_keys:\n",
    "            continue\n",
    "        with mlflow.start_run(run_name=run_key):\n",
    "            mlflow.log_param(\"file_name\", f)\n",
    "            mlflow.log_param(\"model_type\", m_name)\n",
    "            log_dataset(df, f)\n",
    "            gs = GridSearchCV(Cls(), param_grid, cv=3, scoring=\"accuracy\", n_jobs=1)\n",
    "            gs.fit(X_train, y_train)\n",
    "            best_model = gs.best_estimator_\n",
    "            preds = best_model.predict(X_test)\n",
    "            acc = accuracy_score(y_test, preds)\n",
    "            prec = precision_score(y_test, preds, average=\"weighted\")\n",
    "            rec = recall_score(y_test, preds, average=\"weighted\")\n",
    "            f1 = f1_score(y_test, preds, average=\"weighted\")\n",
    "            mlflow.log_params(gs.best_params_)\n",
    "            mlflow.log_metric(\"accuracy\", acc)\n",
    "            mlflow.log_metric(\"precision\", prec)\n",
    "            mlflow.log_metric(\"recall\", rec)\n",
    "            mlflow.log_metric(\"f1_score\", f1)\n",
    "            mlflow.sklearn.log_model(best_model, f\"{m_name}_Model\")\n",
    "            log_confusion_matrix(y_test, preds, run_key, \"ML\")\n",
    "        mlflow.end_run()\n",
    "        new_row = [run_key, f, m_name, acc, prec, rec, f1]\n",
    "        dfp_new = pd.DataFrame([new_row], columns=[\"run_key\", \"File\", \"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
    "        if os.path.exists(progress_file):\n",
    "            dfp_new.to_csv(progress_file, mode='a', index=False, header=False)\n",
    "        else:\n",
    "            dfp_new.to_csv(progress_file, index=False)\n",
    "        processed_keys.add(run_key)\n",
    "\n",
    "# -------------------------------\n",
    "# DL Experiments for Feature Files (using tokenization)\n",
    "# -------------------------------\n",
    "for f in feature_files:\n",
    "    if not os.path.exists(f):\n",
    "        continue\n",
    "    df = pd.read_csv(f)\n",
    "    if \"label\" not in df.columns or \"processed_text\" not in df.columns:\n",
    "        continue\n",
    "    df.dropna(inplace=True)\n",
    "    y = df[\"label\"].values\n",
    "    if y.dtype == object or y.dtype == \"O\":\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "    texts = df[\"processed_text\"].fillna(\"\").astype(str).tolist()\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    vocab_size = 10000\n",
    "    max_length = 200\n",
    "    tokenizer_obj = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "    tokenizer_obj.fit_on_texts(texts)\n",
    "    sequences = tokenizer_obj.texts_to_sequences(texts)\n",
    "    padded = pad_sequences(sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "    X_text = padded\n",
    "    X_train_dl, X_test_dl, y_train_dl, y_test_dl = train_test_split(X_text, y, test_size=0.2, random_state=42)\n",
    "    num_classes = len(np.unique(y))\n",
    "    \n",
    "    # DL Experiment 1: 2-Layer LSTM\n",
    "    dl_run_key = f\"DL_2LSTM_{name_prefix}\"\n",
    "    if dl_run_key not in processed_keys:\n",
    "        with mlflow.start_run(run_name=dl_run_key):\n",
    "            mlflow.log_param(\"file_name\", f)\n",
    "            mlflow.log_param(\"model_type\", \"2-Layer LSTM\")\n",
    "            log_dataset(df, f)\n",
    "            model_dl = Sequential()\n",
    "            model_dl.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length))\n",
    "            model_dl.add(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\n",
    "            model_dl.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "            model_dl.add(Dense(64, activation=\"relu\"))\n",
    "            model_dl.add(Dropout(0.2))\n",
    "            model_dl.add(Dense(num_classes, activation=\"softmax\"))\n",
    "            model_dl.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "            model_dl.fit(X_train_dl, y_train_dl, epochs=3, batch_size=32, validation_split=0.1, verbose=0)\n",
    "            loss_dl, acc_dl = model_dl.evaluate(X_test_dl, y_test_dl, verbose=0)\n",
    "            preds_prob_dl = model_dl.predict(X_test_dl)\n",
    "            preds_dl = preds_prob_dl.argmax(axis=1)\n",
    "            prec_dl = precision_score(y_test_dl, preds_dl, average=\"weighted\")\n",
    "            rec_dl = recall_score(y_test_dl, preds_dl, average=\"weighted\")\n",
    "            f1_dl = f1_score(y_test_dl, preds_dl, average=\"weighted\")\n",
    "            mlflow.log_metric(\"accuracy\", acc_dl)\n",
    "            mlflow.log_metric(\"precision\", prec_dl)\n",
    "            mlflow.log_metric(\"recall\", rec_dl)\n",
    "            mlflow.log_metric(\"f1_score\", f1_dl)\n",
    "            mlflow.keras.log_model(model_dl, \"2LSTM_Model\")\n",
    "            log_confusion_matrix(y_test_dl, preds_dl, dl_run_key, \"DL\")\n",
    "        mlflow.end_run()\n",
    "        new_row = [dl_run_key, f, \"2-Layer LSTM\", acc_dl, prec_dl, rec_dl, f1_dl]\n",
    "        dfp_new = pd.DataFrame([new_row], columns=[\"run_key\", \"File\", \"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
    "        if os.path.exists(progress_file):\n",
    "            dfp_new.to_csv(progress_file, mode='a', index=False, header=False)\n",
    "        else:\n",
    "            dfp_new.to_csv(progress_file, index=False)\n",
    "        processed_keys.add(dl_run_key)\n",
    "    \n",
    "    # DL Experiment 2: Bidirectional LSTM\n",
    "    dl_run_key_bi = f\"DL_BiLSTM_{name_prefix}\"\n",
    "    if dl_run_key_bi not in processed_keys:\n",
    "        with mlflow.start_run(run_name=dl_run_key_bi):\n",
    "            mlflow.log_param(\"file_name\", f)\n",
    "            mlflow.log_param(\"model_type\", \"Bidirectional LSTM\")\n",
    "            log_dataset(df, f)\n",
    "            model_bi = Sequential()\n",
    "            model_bi.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length))\n",
    "            model_bi.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)))\n",
    "            model_bi.add(Dense(64, activation=\"relu\"))\n",
    "            model_bi.add(Dropout(0.2))\n",
    "            model_bi.add(Dense(num_classes, activation=\"softmax\"))\n",
    "            model_bi.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "            model_bi.fit(X_train_dl, y_train_dl, epochs=3, batch_size=32, validation_split=0.1, verbose=0)\n",
    "            loss_bi, acc_bi = model_bi.evaluate(X_test_dl, y_test_dl, verbose=0)\n",
    "            preds_prob_bi = model_bi.predict(X_test_dl)\n",
    "            preds_bi = preds_prob_bi.argmax(axis=1)\n",
    "            prec_bi = precision_score(y_test_dl, preds_bi, average=\"weighted\")\n",
    "            rec_bi = recall_score(y_test_dl, preds_bi, average=\"weighted\")\n",
    "            f1_bi = f1_score(y_test_dl, preds_bi, average=\"weighted\")\n",
    "            mlflow.log_metric(\"accuracy\", acc_bi)\n",
    "            mlflow.log_metric(\"precision\", prec_bi)\n",
    "            mlflow.log_metric(\"recall\", rec_bi)\n",
    "            mlflow.log_metric(\"f1_score\", f1_bi)\n",
    "            mlflow.keras.log_model(model_bi, \"BiLSTM_Model\")\n",
    "            log_confusion_matrix(y_test_dl, preds_bi, dl_run_key_bi, \"DL\")\n",
    "        mlflow.end_run()\n",
    "        new_row = [dl_run_key_bi, f, \"Bidirectional LSTM\", acc_bi, prec_bi, rec_bi, f1_bi]\n",
    "        dfp_new = pd.DataFrame([new_row], columns=[\"run_key\", \"File\", \"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
    "        if os.path.exists(progress_file):\n",
    "            dfp_new.to_csv(progress_file, mode='a', index=False, header=False)\n",
    "        else:\n",
    "            dfp_new.to_csv(progress_file, index=False)\n",
    "        processed_keys.add(dl_run_key_bi)\n",
    "\n",
    "# ------------------------------\n",
    "# DL Experiments for Embedding Files (using precomputed embeddings)\n",
    "# ------------------------------\n",
    "for f in embedding_files:\n",
    "    if not os.path.exists(f):\n",
    "        continue\n",
    "    df = pd.read_csv(f)\n",
    "    if \"label\" not in df.columns:\n",
    "        continue\n",
    "    df.dropna(inplace=True)\n",
    "    y = df[\"label\"].values\n",
    "    if y.dtype == object or y.dtype == \"O\":\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "    # Assume precomputed embeddings are all numeric features except the label.\n",
    "    X = df.drop(columns=[\"label\"]).values\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X_train_e, X_test_e, y_train_e, y_test_e = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    name_prefix = os.path.basename(f)\n",
    "    dl_run_key_dense = f\"DL_Dense_{name_prefix}\"\n",
    "    if dl_run_key_dense not in processed_keys:\n",
    "        with mlflow.start_run(run_name=dl_run_key_dense):\n",
    "            mlflow.log_param(\"file_name\", f)\n",
    "            mlflow.log_param(\"model_type\", \"Dense NN on Embeddings\")\n",
    "            log_dataset(df, f)\n",
    "            input_dim = X.shape[1]\n",
    "            model_dense = Sequential()\n",
    "            model_dense.add(Dense(128, activation=\"relu\", input_dim=input_dim))\n",
    "            model_dense.add(Dropout(0.2))\n",
    "            model_dense.add(Dense(64, activation=\"relu\"))\n",
    "            model_dense.add(Dropout(0.2))\n",
    "            model_dense.add(Dense(len(np.unique(y)), activation=\"softmax\"))\n",
    "            model_dense.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "            model_dense.fit(X_train_e, y_train_e, epochs=3, batch_size=32, validation_split=0.1, verbose=0)\n",
    "            loss_dense, acc_dense = model_dense.evaluate(X_test_e, y_test_e, verbose=0)\n",
    "            preds_dense = model_dense.predict(X_test_e).argmax(axis=1)\n",
    "            prec_dense = precision_score(y_test_e, preds_dense, average=\"weighted\")\n",
    "            rec_dense = recall_score(y_test_e, preds_dense, average=\"weighted\")\n",
    "            f1_dense = f1_score(y_test_e, preds_dense, average=\"weighted\")\n",
    "            mlflow.log_metric(\"accuracy\", acc_dense)\n",
    "            mlflow.log_metric(\"precision\", prec_dense)\n",
    "            mlflow.log_metric(\"recall\", rec_dense)\n",
    "            mlflow.log_metric(\"f1_score\", f1_dense)\n",
    "            mlflow.keras.log_model(model_dense, \"DenseNN_Model\")\n",
    "            log_confusion_matrix(y_test_e, preds_dense, dl_run_key_dense, \"DL\")\n",
    "        mlflow.end_run()\n",
    "        new_row = [dl_run_key_dense, f, \"Dense NN on Embeddings\", acc_dense, prec_dense, rec_dense, f1_dense]\n",
    "        dfp_new = pd.DataFrame([new_row], columns=[\"run_key\", \"File\", \"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
    "        if os.path.exists(progress_file):\n",
    "            dfp_new.to_csv(progress_file, mode='a', index=False, header=False)\n",
    "        else:\n",
    "            dfp_new.to_csv(progress_file, index=False)\n",
    "        processed_keys.add(dl_run_key_dense)\n",
    "\n",
    "print(\"All experiments completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
