{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"pip install -q dagshub mlflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not delete run 2d0257df3f4f44ec8e7115bf59e61c85: API request to endpoint /api/2.0/mlflow/runs/delete failed with error code 403 != 200. Response body: ''\n",
      "Could not delete run 51a8715a37c444a19ef3a9fc0018c6b0: API request to endpoint /api/2.0/mlflow/runs/delete failed with error code 403 != 200. Response body: ''\n",
      "Could not delete run 4a870c3ae9d646909c750b4fd09c184a: API request to endpoint /api/2.0/mlflow/runs/delete failed with error code 403 != 200. Response body: ''\n",
      "Could not delete run 3f12b79a5b6c46d1acb31d5b271370ab: API request to endpoint /api/2.0/mlflow/runs/delete failed with error code 403 != 200. Response body: ''\n",
      "Could not delete run 9f603c8dd2874928b176cb3afcede509: API request to endpoint /api/2.0/mlflow/runs/delete failed with error code 403 != 200. Response body: ''\n",
      "Could not delete run 94877f52e89442e0b16c0ace287ee091: API request to endpoint /api/2.0/mlflow/runs/delete failed with error code 403 != 200. Response body: ''\n",
      "Could not delete run db45689c2a8c4640aea50303e42b27c7: API request to endpoint /api/2.0/mlflow/runs/delete failed with error code 403 != 200. Response body: ''\n",
      "Could not delete run 1f2394fc7c8a405aba1cd27601fb54e3: API request to endpoint /api/2.0/mlflow/runs/delete failed with error code 403 != 200. Response body: ''\n",
      "Could not delete run 46612e028ebc4525b95d0622fef63dae: API request to endpoint /api/2.0/mlflow/runs/delete failed with error code 403 != 200. Response body: ''\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "mlflow.set_tracking_uri(\"https://dagshub.com/malhar.c.prajapati/my-first-repo.mlflow\")\n",
    "client = MlflowClient()\n",
    "default_experiment = client.get_experiment_by_name(\"Fake Review Detection\")\n",
    "runs = client.search_runs(experiment_ids=[default_experiment.experiment_id])\n",
    "for run in runs:\n",
    "    try:\n",
    "        client.delete_run(run.info.run_id)\n",
    "        print(f\"Deleted run {run.info.run_id} from experiment {default_experiment.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not delete run {run.info.run_id}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/06 21:36:12 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run preprocessed_lemmatization_features.csv_Tfidf_LogisticRegression at: https://dagshub.com/malhar.c.prajapati/my-first-repo.mlflow/#/experiments/1/runs/4a870c3ae9d646909c750b4fd09c184a\n",
      "üß™ View experiment at: https://dagshub.com/malhar.c.prajapati/my-first-repo.mlflow/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/06 22:02:33 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run preprocessed_lemmatization_features.csv_Tfidf_RandomForest at: https://dagshub.com/malhar.c.prajapati/my-first-repo.mlflow/#/experiments/1/runs/51a8715a37c444a19ef3a9fc0018c6b0\n",
      "üß™ View experiment at: https://dagshub.com/malhar.c.prajapati/my-first-repo.mlflow/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.keras\n",
    "import mlflow.data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "\n",
    "# Set new experiment name and MLflow tracking parameters\n",
    "NEW_EXPERIMENT_NAME = \"Fake Review Detection 2.0\"\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = os.getenv(\"MLFLOW_TRACKING_USERNAME\", \"malhar.c.prajapati\")\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = os.getenv(\"MLFLOW_TRACKING_PASSWORD\", \"f222587ea4fa84ee148e478d207d3112535c5edd\")\n",
    "mlflow.set_tracking_uri(\"https://dagshub.com/malhar.c.prajapati/my-first-repo.mlflow\")\n",
    "mlflow.set_experiment(NEW_EXPERIMENT_NAME)\n",
    "\n",
    "# Define file lists\n",
    "feature_files = [\n",
    "    \"../Data/Feature-Engineered/preprocessed_lemmatization_features.csv\",\n",
    "    \"../Data/Feature-Engineered/preprocessed_no_stopwords_features.csv\",\n",
    "    \"../Data/Feature-Engineered/preprocessed_no_stopwords_no_lemmatization_features.csv\",\n",
    "    \"../Data/Feature-Engineered/preprocessed_stemming_features.csv\",\n",
    "    \"../Data/Feature-Engineered/preprocessed_stemming_no_stopwords_features.csv\"\n",
    "]\n",
    "\n",
    "embedding_files = [\n",
    "    \"../../embeddings/preprocessed_lemmatization_bert.csv\",\n",
    "    \"../../embeddings/preprocessed_lemmatization_glove.csv\",\n",
    "    \"../../embeddings/preprocessed_lemmatization_tfidf.csv\",\n",
    "    \"../../embeddings/preprocessed_no_stopwords_bert.csv\",\n",
    "    \"../../embeddings/preprocessed_no_stopwords_glove.csv\",\n",
    "    \"../../embeddings/preprocessed_no_stopwords_no_lemmatization_bert.csv\",\n",
    "    \"../../embeddings/preprocessed_no_stopwords_no_lemmatization_glove.csv\",\n",
    "    \"../../embeddings/preprocessed_no_stopwords_no_lemmatization_tfidf.csv\",\n",
    "    \"../../embeddings/preprocessed_no_stopwords_tfidf.csv\",\n",
    "    \"../../embeddings/preprocessed_stemming_bert.csv\",\n",
    "    \"../../embeddings/preprocessed_stemming_glove.csv\",\n",
    "    \"../../embeddings/preprocessed_stemming_no_stopwords_bert.csv\",\n",
    "    \"../../embeddings/preprocessed_stemming_no_stopwords_glove.csv\",\n",
    "    \"../../embeddings/preprocessed_stemming_no_stopwords_tfidf.csv\",\n",
    "    \"../../embeddings/preprocessed_stemming_tfidf.csv\"\n",
    "]\n",
    "\n",
    "files = feature_files + embedding_files\n",
    "\n",
    "# Define ML models and their parameter grids\n",
    "models = {\n",
    "    \"LogisticRegression\": (LogisticRegression, {\"C\": [0.1, 1], \"solver\": [\"liblinear\"], \"max_iter\": [100]}),\n",
    "    \"RandomForest\": (RandomForestClassifier, {\"n_estimators\": [50, 100], \"max_depth\": [None, 10]}),\n",
    "    \"SVC\": (SVC, {\"C\": [0.1, 1], \"kernel\": [\"linear\"]})\n",
    "}\n",
    "\n",
    "progress_file = \"progress_log.csv\"\n",
    "if os.path.exists(progress_file):\n",
    "    dfp = pd.read_csv(progress_file)\n",
    "    processed_keys = set(dfp[\"run_key\"].tolist())\n",
    "else:\n",
    "    processed_keys = set()\n",
    "\n",
    "def log_confusion_matrix(y_true, y_pred, run_key, prefix):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    cm_path = f\"../Reports/confusion_matrix_{prefix}_{run_key}.png\"\n",
    "    plt.savefig(cm_path)\n",
    "    mlflow.log_artifact(cm_path)\n",
    "    plt.close()\n",
    "\n",
    "def log_dataset(df, source_file):\n",
    "    try:\n",
    "        ds = mlflow.data.from_pandas(df, source=source_file)\n",
    "        mlflow.data.log_dataset(ds, name=\"embedding_data\")\n",
    "    except Exception:\n",
    "        mlflow.log_artifact(source_file, artifact_path=\"dataset_csv\")\n",
    "\n",
    "# ------------------------------\n",
    "# Traditional ML Experiments (applied to all files)\n",
    "# ------------------------------\n",
    "for f in files:\n",
    "    if not os.path.exists(f):\n",
    "        continue\n",
    "    df = pd.read_csv(f)\n",
    "    if \"label\" not in df.columns:\n",
    "        continue\n",
    "    df.dropna(inplace=True)\n",
    "    y = df[\"label\"].values\n",
    "    if y.dtype == object or y.dtype == \"O\":\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "    # For feature files with computed metrics, use numeric columns; otherwise use all columns except label.\n",
    "    if f in feature_files and \"processed_text\" in df.columns and \"lexical_diversity\" in df.columns:\n",
    "        numeric_cols = [\"lexical_diversity\", \"avg_word_length\", \"sentiment_polarity\",\n",
    "                        \"subjectivity\", \"flesch_reading_ease\", \"sentence_length\",\n",
    "                        \"named_entity_count\", \"noun_count\", \"verb_count\", \"adj_count\", \"adv_count\"]\n",
    "        available_cols = [col for col in numeric_cols if col in df.columns]\n",
    "        X = df[available_cols].values\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    else:\n",
    "        X = df.drop(columns=[\"label\"]).values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    name_prefix = os.path.basename(f)\n",
    "    for m_name, (Cls, param_grid) in models.items():\n",
    "        run_key = f\"{name_prefix}_{m_name}_ML\"\n",
    "        if run_key in processed_keys:\n",
    "            continue\n",
    "        with mlflow.start_run(run_name=run_key):\n",
    "            mlflow.log_param(\"file_name\", f)\n",
    "            mlflow.log_param(\"model_type\", m_name)\n",
    "            log_dataset(df, f)\n",
    "            gs = GridSearchCV(Cls(), param_grid, cv=3, scoring=\"accuracy\", n_jobs=1)\n",
    "            gs.fit(X_train, y_train)\n",
    "            best_model = gs.best_estimator_\n",
    "            preds = best_model.predict(X_test)\n",
    "            acc = accuracy_score(y_test, preds)\n",
    "            prec = precision_score(y_test, preds, average=\"weighted\")\n",
    "            rec = recall_score(y_test, preds, average=\"weighted\")\n",
    "            f1 = f1_score(y_test, preds, average=\"weighted\")\n",
    "            mlflow.log_params(gs.best_params_)\n",
    "            mlflow.log_metric(\"accuracy\", acc)\n",
    "            mlflow.log_metric(\"precision\", prec)\n",
    "            mlflow.log_metric(\"recall\", rec)\n",
    "            mlflow.log_metric(\"f1_score\", f1)\n",
    "            mlflow.sklearn.log_model(best_model, f\"{m_name}_Model\")\n",
    "            log_confusion_matrix(y_test, preds, run_key, \"ML\")\n",
    "        mlflow.end_run()\n",
    "        new_row = [run_key, f, m_name, acc, prec, rec, f1]\n",
    "        dfp_new = pd.DataFrame([new_row], columns=[\"run_key\", \"File\", \"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
    "        if os.path.exists(progress_file):\n",
    "            dfp_new.to_csv(progress_file, mode='a', index=False, header=False)\n",
    "        else:\n",
    "            dfp_new.to_csv(progress_file, index=False)\n",
    "        processed_keys.add(run_key)\n",
    "\n",
    "# ------------------------------\n",
    "# DL Experiments for Feature Files (using tokenization)\n",
    "# ------------------------------\n",
    "for f in feature_files:\n",
    "    if not os.path.exists(f):\n",
    "        continue\n",
    "    df = pd.read_csv(f)\n",
    "    if \"label\" not in df.columns or \"processed_text\" not in df.columns:\n",
    "        continue\n",
    "    df.dropna(inplace=True)\n",
    "    y = df[\"label\"].values\n",
    "    if y.dtype == object or y.dtype == \"O\":\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "    texts = df[\"processed_text\"].fillna(\"\").astype(str).tolist()\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    vocab_size = 10000\n",
    "    max_length = 200\n",
    "    tokenizer_obj = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "    tokenizer_obj.fit_on_texts(texts)\n",
    "    sequences = tokenizer_obj.texts_to_sequences(texts)\n",
    "    padded = pad_sequences(sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "    X_text = padded\n",
    "    X_train_dl, X_test_dl, y_train_dl, y_test_dl = train_test_split(X_text, y, test_size=0.2, random_state=42)\n",
    "    num_classes = len(np.unique(y))\n",
    "    \n",
    "    # DL Experiment 1: 2-Layer LSTM\n",
    "    dl_run_key = f\"DL_2LSTM_{name_prefix}\"\n",
    "    if dl_run_key not in processed_keys:\n",
    "        with mlflow.start_run(run_name=dl_run_key):\n",
    "            mlflow.log_param(\"file_name\", f)\n",
    "            mlflow.log_param(\"model_type\", \"2-Layer LSTM\")\n",
    "            log_dataset(df, f)\n",
    "            model_dl = Sequential()\n",
    "            model_dl.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length))\n",
    "            model_dl.add(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\n",
    "            model_dl.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "            model_dl.add(Dense(64, activation=\"relu\"))\n",
    "            model_dl.add(Dropout(0.2))\n",
    "            model_dl.add(Dense(num_classes, activation=\"softmax\"))\n",
    "            model_dl.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "            model_dl.fit(X_train_dl, y_train_dl, epochs=3, batch_size=32, validation_split=0.1, verbose=0)\n",
    "            loss_dl, acc_dl = model_dl.evaluate(X_test_dl, y_test_dl, verbose=0)\n",
    "            preds_prob_dl = model_dl.predict(X_test_dl)\n",
    "            preds_dl = preds_prob_dl.argmax(axis=1)\n",
    "            prec_dl = precision_score(y_test_dl, preds_dl, average=\"weighted\")\n",
    "            rec_dl = recall_score(y_test_dl, preds_dl, average=\"weighted\")\n",
    "            f1_dl = f1_score(y_test_dl, preds_dl, average=\"weighted\")\n",
    "            mlflow.log_metric(\"accuracy\", acc_dl)\n",
    "            mlflow.log_metric(\"precision\", prec_dl)\n",
    "            mlflow.log_metric(\"recall\", rec_dl)\n",
    "            mlflow.log_metric(\"f1_score\", f1_dl)\n",
    "            mlflow.keras.log_model(model_dl, \"2LSTM_Model\")\n",
    "            log_confusion_matrix(y_test_dl, preds_dl, dl_run_key, \"DL\")\n",
    "        mlflow.end_run()\n",
    "        new_row = [dl_run_key, f, \"2-Layer LSTM\", acc_dl, prec_dl, rec_dl, f1_dl]\n",
    "        dfp_new = pd.DataFrame([new_row], columns=[\"run_key\", \"File\", \"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
    "        if os.path.exists(progress_file):\n",
    "            dfp_new.to_csv(progress_file, mode='a', index=False, header=False)\n",
    "        else:\n",
    "            dfp_new.to_csv(progress_file, index=False)\n",
    "        processed_keys.add(dl_run_key)\n",
    "    \n",
    "    # DL Experiment 2: Bidirectional LSTM\n",
    "    dl_run_key_bi = f\"DL_BiLSTM_{name_prefix}\"\n",
    "    if dl_run_key_bi not in processed_keys:\n",
    "        with mlflow.start_run(run_name=dl_run_key_bi):\n",
    "            mlflow.log_param(\"file_name\", f)\n",
    "            mlflow.log_param(\"model_type\", \"Bidirectional LSTM\")\n",
    "            log_dataset(df, f)\n",
    "            model_bi = Sequential()\n",
    "            model_bi.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length))\n",
    "            model_bi.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)))\n",
    "            model_bi.add(Dense(64, activation=\"relu\"))\n",
    "            model_bi.add(Dropout(0.2))\n",
    "            model_bi.add(Dense(num_classes, activation=\"softmax\"))\n",
    "            model_bi.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "            model_bi.fit(X_train_dl, y_train_dl, epochs=3, batch_size=32, validation_split=0.1, verbose=0)\n",
    "            loss_bi, acc_bi = model_bi.evaluate(X_test_dl, y_test_dl, verbose=0)\n",
    "            preds_prob_bi = model_bi.predict(X_test_dl)\n",
    "            preds_bi = preds_prob_bi.argmax(axis=1)\n",
    "            prec_bi = precision_score(y_test_dl, preds_bi, average=\"weighted\")\n",
    "            rec_bi = recall_score(y_test_dl, preds_bi, average=\"weighted\")\n",
    "            f1_bi = f1_score(y_test_dl, preds_bi, average=\"weighted\")\n",
    "            mlflow.log_metric(\"accuracy\", acc_bi)\n",
    "            mlflow.log_metric(\"precision\", prec_bi)\n",
    "            mlflow.log_metric(\"recall\", rec_bi)\n",
    "            mlflow.log_metric(\"f1_score\", f1_bi)\n",
    "            mlflow.keras.log_model(model_bi, \"BiLSTM_Model\")\n",
    "            log_confusion_matrix(y_test_dl, preds_bi, dl_run_key_bi, \"DL\")\n",
    "        mlflow.end_run()\n",
    "        new_row = [dl_run_key_bi, f, \"Bidirectional LSTM\", acc_bi, prec_bi, rec_bi, f1_bi]\n",
    "        dfp_new = pd.DataFrame([new_row], columns=[\"run_key\", \"File\", \"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
    "        if os.path.exists(progress_file):\n",
    "            dfp_new.to_csv(progress_file, mode='a', index=False, header=False)\n",
    "        else:\n",
    "            dfp_new.to_csv(progress_file, index=False)\n",
    "        processed_keys.add(dl_run_key_bi)\n",
    "\n",
    "# ------------------------------\n",
    "# DL Experiments for Embedding Files (using precomputed embeddings)\n",
    "# ------------------------------\n",
    "for f in embedding_files:\n",
    "    if not os.path.exists(f):\n",
    "        continue\n",
    "    df = pd.read_csv(f)\n",
    "    if \"label\" not in df.columns:\n",
    "        continue\n",
    "    df.dropna(inplace=True)\n",
    "    y = df[\"label\"].values\n",
    "    if y.dtype == object or y.dtype == \"O\":\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "    # Assume precomputed embeddings are all numeric features except the label.\n",
    "    X = df.drop(columns=[\"label\"]).values\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X_train_e, X_test_e, y_train_e, y_test_e = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    name_prefix = os.path.basename(f)\n",
    "    dl_run_key_dense = f\"DL_Dense_{name_prefix}\"\n",
    "    if dl_run_key_dense not in processed_keys:\n",
    "        with mlflow.start_run(run_name=dl_run_key_dense):\n",
    "            mlflow.log_param(\"file_name\", f)\n",
    "            mlflow.log_param(\"model_type\", \"Dense NN on Embeddings\")\n",
    "            log_dataset(df, f)\n",
    "            input_dim = X.shape[1]\n",
    "            model_dense = Sequential()\n",
    "            model_dense.add(Dense(128, activation=\"relu\", input_dim=input_dim))\n",
    "            model_dense.add(Dropout(0.2))\n",
    "            model_dense.add(Dense(64, activation=\"relu\"))\n",
    "            model_dense.add(Dropout(0.2))\n",
    "            model_dense.add(Dense(len(np.unique(y)), activation=\"softmax\"))\n",
    "            model_dense.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "            model_dense.fit(X_train_e, y_train_e, epochs=3, batch_size=32, validation_split=0.1, verbose=0)\n",
    "            loss_dense, acc_dense = model_dense.evaluate(X_test_e, y_test_e, verbose=0)\n",
    "            preds_dense = model_dense.predict(X_test_e).argmax(axis=1)\n",
    "            prec_dense = precision_score(y_test_e, preds_dense, average=\"weighted\")\n",
    "            rec_dense = recall_score(y_test_e, preds_dense, average=\"weighted\")\n",
    "            f1_dense = f1_score(y_test_e, preds_dense, average=\"weighted\")\n",
    "            mlflow.log_metric(\"accuracy\", acc_dense)\n",
    "            mlflow.log_metric(\"precision\", prec_dense)\n",
    "            mlflow.log_metric(\"recall\", rec_dense)\n",
    "            mlflow.log_metric(\"f1_score\", f1_dense)\n",
    "            mlflow.keras.log_model(model_dense, \"DenseNN_Model\")\n",
    "            log_confusion_matrix(y_test_e, preds_dense, dl_run_key_dense, \"DL\")\n",
    "        mlflow.end_run()\n",
    "        new_row = [dl_run_key_dense, f, \"Dense NN on Embeddings\", acc_dense, prec_dense, rec_dense, f1_dense]\n",
    "        dfp_new = pd.DataFrame([new_row], columns=[\"run_key\", \"File\", \"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
    "        if os.path.exists(progress_file):\n",
    "            dfp_new.to_csv(progress_file, mode='a', index=False, header=False)\n",
    "        else:\n",
    "            dfp_new.to_csv(progress_file, index=False)\n",
    "        processed_keys.add(dl_run_key_dense)\n",
    "\n",
    "print(\"All experiments completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
