{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"pip install import-ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "import re\n",
    "import string\n",
    "import inflect\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "from langdetect import detect\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_translate(text):\n",
    "    try:\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return \"Invalid or Empty Text\"\n",
    "        return GoogleTranslator(source='auto', target='en').translate(text)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Deep Translator Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize necessary components\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = \"../Assets/frequency_dictionary_en_82_765.txt\"\n",
    "if not sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1):\n",
    "    raise FileNotFoundError(f\"SymSpell dictionary file not found at {dictionary_path}\")\n",
    "\n",
    "inflect_engine = inflect.engine()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\"Detect the language of a given text, returning 'unknown' if detection fails.\"\"\"\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "def convert_numbers(text):\n",
    "    \"\"\"Convert numeric digits to their word representation (e.g., '4' → 'four').\"\"\"\n",
    "    return re.sub(r'\\b\\d+\\b', lambda x: inflect_engine.number_to_words(x.group()), text)\n",
    "\n",
    "def correct_spelling(word):\n",
    "    \"\"\"Correct spelling using SymSpell, returning the closest suggestion if available.\"\"\"\n",
    "    suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "    return suggestions[0].term if suggestions else word\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map NLTK POS tags to WordNet POS for better lemmatization.\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def preprocess_text(text, remove_stopwords=False, use_stemming=False, use_lemmatization=True):\n",
    "    \"\"\"Preprocess text by normalizing, cleaning, tokenizing, correcting, and optionally lemmatizing/stemming.\"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "\n",
    "    text = convert_numbers(text)  # Convert numbers to words\n",
    "\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove punctuation\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    words = [correct_spelling(word) for word in words]  # Correct spelling\n",
    "\n",
    "    if remove_stopwords:\n",
    "        words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
    "\n",
    "    if use_stemming:\n",
    "        words = [stemmer.stem(word) for word in words]  # Stemming\n",
    "\n",
    "    if use_lemmatization:\n",
    "        words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]  # Lemmatization\n",
    "\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "# def translate_text(text):\n",
    "#     return deep_translate(text)\n",
    "\n",
    "# def preprocess_single_text(text, remove_stopwords=False, use_stemming=False, use_lemmatization=True):\n",
    "#     text = preprocess_text(text, remove_stopwords, use_stemming, use_lemmatization)\n",
    "#     return text\n",
    "\n",
    "# def feature_extract_single_text(text, vectorizer):\n",
    "#     text_tfidf = vectorizer.transform([text])\n",
    "#     return text_tfidf\n",
    "\n",
    "# def predict_single_text(text, vectorizer, model, remove_stopwords=False, use_stemming=False, use_lemmatization=True):\n",
    "#     translated_text = translate_text(text)\n",
    "#     preprocessed_text = preprocess_single_text(translated_text, remove_stopwords, use_stemming, use_lemmatization)\n",
    "#     text_tfidf = feature_extract_single_text(preprocessed_text, vectorizer)\n",
    "#     prediction = model.predict(text_tfidf)\n",
    "#     return prediction\n",
    "\n",
    "# def run_single_text_pipeline(text):\n",
    "#     vectorizer = joblib.load('vectorizer.pkl')  \n",
    "#     model = joblib.load('model.pkl')  \n",
    "\n",
    "#     prediction = predict_single_text(text, vectorizer, model)\n",
    "\n",
    "#     print(\"✅ Prediction completed successfully!\")\n",
    "#     return prediction\n",
    "\n",
    "# # Example usage:\n",
    "# text = \"Enter the text here for prediction\"\n",
    "# prediction = run_single_text_pipeline(text)\n",
    "# print(f\"Prediction: {prediction}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
